{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tell me about yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My name is Jasraj, but most people call me Jas. I was born and raised in New Delhi, India, though a big part of my extended family is from Punjab, so I like to think of myself as a mix of the two cultures.\n",
    "\n",
    "For college, I went to Nanyang Technological University in Singapore, where I studied Mathematical and Computer Sciences. I wanted to study math; my parents wanted computer science. So, this was our middle ground. It turned out great because it’s where I got into theoretical machine learning. I did my thesis research with Professor Bryan Low at the National University of Singapore, working on active learning for robust neural network training  in low-data settings.\n",
    "\n",
    "In 2023, I started my Master’s in Machine Learning at UCL. I went in with a plan to specialize in optimization, dynamical systems, and probabilistic ML, while using other courses to round out my theoretical foundation. My thesis research there was supervised by Professor Laura Toni and Professor Brooks Paige, where we analyzed methods designed for training deep GNNs in the context of long-range tasks.\n",
    "\n",
    "Outside of academics, I used to play basketball seriously—three years at the national level in high school—until I tore the ligament and meniscus in my knee, which shifted my focus to other things. I’m now really into weightlifting—it’s something I try to stay consistent with. Back in high school, I used to run a lot, but I’ve let that slide over the years, so I’m also working on building my cardiovascular fitness again.\n",
    "\n",
    "That’s a little about me; I’m happy to dive deeper into any part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first major research project was during my bachelor’s thesis at the National University of Singapore, where I worked with a Ph.D. student on active learning. We noticed that many existing algorithms, while effective, were highly sensitive to model initialization—an issue that can be problematic in critical applications like healthcare. Using NTK theory, we developed an algorithm that minimizes predictive variance, making it more robust to initialization. We also proved that our data selction criterion forms a theoretical upper bound on the test error, guaranteeing its performance. Our approach performed better than state-of-the-art methods, especially with limited data, and was published at ICML 2023.\n",
    "\n",
    "Upon graduation, I joined the Visual and Cognitive Neuroscience Lab at NTU to explore linguistic analysis for fake news detection. I analyzed over 100 linguistic features across 10K samples, identifying 18 key features with significantly different distributions across real and fake news. Incorporating these features improved the fine-tuning performance of 11 large language models on COVID-19 fake news datasets, reducing error rates by up to 18%. Our approach was competitive with state-of-the-art methods while being much more data- and compute-efficient. This work is currently under review at TMLR.\n",
    "\n",
    "I'm currently working with the Learning and Signal Processing Lab at UCL, continuing my MSc thesis project on analyzing the limitations of graph neural networks in long-range tasks. I found that popular techniques like random edge-dropping and residual connections, which are designed to train deep GNNs, actually worsen the over-squashing problem, making it harder for models to capture long-range interactions. I supported this with both theoretical analysis and experiments on real-world long-range tasks, showing a clear decline in performance. This work is under review for ICLR 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tell me about your current research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm currently working with the Learning and Signal Processing Lab at UCL, continuing my MSc thesis project under the supervision of Professor Laura Toni and Professor Brooks Paige. We are analyzing methods designed for training deep GNNs in the context of long-range tasks.\n",
    "\n",
    "To give some background, training deep GNNs is challenging because of issues like over-smoothing. Over-smoothing happens when node representations become too similar as the number of message-passing steps increases. This makes the model lose its expressiveness and limits its ability to handle graph data effectively.\n",
    "\n",
    "Recently, another issue, over-squashing, has come to light. Over-squashing happens when information gets bottlenecked as it travels long distances in the graph. This makes it hard for the model to capture long-range interactions, which is the intended use-case of deep GNNs.\n",
    "\n",
    "In my research, we started by identifying two key issues. First, there’s a trade-off between over-smoothing and over-squashing, and second, most methods designed for deep GNNs are evaluated on short-range tasks, even though deep GNNs are meant for long-range tasks. This makes their evaluation somewhat misaligned with their intended purpose.\n",
    "\n",
    "In my MSc thesis, I showed that DropEdge, a common method for reducing over-smoothing, actually makes over-squashing worse and leads to poor performance on long-range tasks. For our ICLR submission, I expanded this work to analyze three other dropout-like methods and found they had a similar negative impact on long-range tasks.\n",
    "\n",
    "The feedback we received from reviewers was encouraging—they appreciated the problem we highlighted and how we presented it. A common concern, though, was that the range of algorithms we analyzed was somewhat limited. Right now, I’m focusing on extending this work to include other popular methods that face the same issue. \n",
    "\n",
    "One issue I have run into is that for methods resulting in performance decline, it is easy to conclude their negative effects on over-squashing since performance is dropping despite the alleviation of over-smoothing. However, for some other methods, like residual connections, the performance may improve despite the exacerbation of the over-squashing problem, since the benefits of alleviating over-smoothing are a lot more significant. I am currently trying to figure out how to distangle the two effects to conclude the suitability of such methods towards long-range tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do you want to do a PhD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to pursue a PhD because I’m deeply motivated to address inefficiencies in deep learning, particularly in terms of data and compute. I’ve always been someone who dislikes redundancy and inefficiency, whether it’s in coding, proofs, or processes. In deep learning, this inefficiency is evident in how much data and computational power models require, even for simple tasks. For example, OpenAI’s o3 model on ARC-AGI costs thousands of dollars per problem, despite the problems being simple for humans. This makes scaling and deploying deep learning challenging, especially in low-resource settings where these inefficiencies can’t be offset by hardware. I’m passionate about tackling these challenges to make deep learning more accessible and practical.\n",
    "\n",
    "A PhD also aligns with my long-term goal of pursuing a research career. I’m currently exploring whether I want to focus on academic or industrial research, and I see a PhD as a great opportunity to learn more about both paths. For example, I know that academic research often includes teaching, which I think I will enjoy. I’ve served as a TA for two statistics modules at NTU and as the head tutor for the Machine Intelligence and Brain Research winter school at IIT Madras. I really enjoyed breaking down technical concepts for students and helping them understand complex ideas. During my PhD, I’d like to explore similar opportunities, especially graduate-level tutoring, to better understand my strengths and areas for growth in teaching.\n",
    "\n",
    "Overall, a PhD feels like the right step for me to dive deeper into the kind of research I care about, and also gain more clarity about my career direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you enjoy about research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enjoy research because it’s about diving into interesting and challenging questions. There’s something incredibly satisfying about breaking down a complex problem, and exploring it from different angles to build a deeper understanding. I find it even more fun when collaborating. It often speeds up the flow of ideas, leading to faster development and more iterations, which makes the whole process more productive. I also think of it as a way to socialize at work.\n",
    "\n",
    "Beyond the immediate work, I find it rewarding to contribute to science in some small way, knowing that my research is part of a much larger effort to model intelligence. For me, the long-term vision is what ties it all together. I imagine a future where we use AI and technology to handle routine tasks, freeing up time for what really matters—human connection, care, and creativity. Thinking about how my work might help us take even a small step toward that future keeps me focused and motivated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am broadly interested in the mathematics of deep learning, with a focus on improving the data and compute efficiency of algorithms. My interests encompass optimization algorithms and their interaction with complex loss landscapes, connections between neural networks, Gaussian processes, and kernel methods, as well as automating deep learning workflows. A central question that captivates me is how simple gradient descent-based algorithms can successfully optimize highly non-convex neural networks.\n",
    "\n",
    "In this context, I greatly appreciated your ICML 2023 work with Amirkeivan on the role of large learning rates in avoiding sharp minima. This aligns closely with my interests in understanding optimization trajectories. Specifically, I am intrigued by the geometry of these trajectories and how their smoothness evolves across different time scales. As I mentioned in my ELLIS PhD program statement of purpose, my ideas are motivated by the edge-of-stability phenomenon and the unstable convergence of the loss under gradient descent.\n",
    "\n",
    "I hypothesize that a similar phenomenon occurs in the optimization steps themselves: while consecutive updates may appear orthogonal, parameter changes over longer intervals exhibit better alignment. This suggests a relatively smooth central flow along which the trajectory aligns, with oscillations occurring perpendicular to it. I aim to explore the prevalence of this pattern and develop mathematical models to better understand it.\n",
    "\n",
    "On the application side, I am excited about potential strategies to accelerate training by optimizing smooth approximations of the true objective. By mitigating rough local geometry around parameter estimates, larger and more reliable steps could be taken during optimization. I am particularly interested in leveraging existing frameworks that model gradient descent dynamics to further these goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
