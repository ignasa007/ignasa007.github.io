<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jasraj Singh" />
  <title>Kernel Methods for Neural Architecture Search</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernel Methods for Neural Architecture Search</h1>
<p class="author">Jasraj Singh</p>
<p class="date">12 June, 2025</p>
</header>
<h1 id="summary">Summary</h1>
<p>The aim of this project was to propose a principled (training-free)
metric for scoring models on a given dataset, thereby introducing a new
strategy for Neural Architecture Search (NAS). The score is defined as
the (kernel) canonical correlation <span class="citation"
data-cites="akaho2007kcca thomas2001kcca">(Akaho, 2007; Melzer, Reiter,
&amp; Bischof, 2001)</span> between the inputs, <span
class="math inline">\(\mathbf{X}\)</span>, and the outputs, <span
class="math inline">\(\mathbf{Y}\)</span>, with respect to the
Reproducing Kernel Hilbert Spaces (RKHS) corresponding to the Neural
Tangent Kernel (NTK) and the linear kernel, respectively. We provide a
theoretical motivation for this metric, and aim to validate its efficacy
on NAS benchmarks: NAS-Bench-201 <span class="citation"
data-cites="dong2020nasbench201">(Dong &amp; Yang, 2020)</span> and
DARTS <span class="citation" data-cites="liu2018darts">(Liu, Simonyan,
&amp; Yang, 2019)</span>.</p>
<h1 id="introduction">Introduction</h1>
<p>Selecting Neural Network (NN) architectures for a task has
traditionally been a manual, time-intensive process requiring domain
expertise and extensive trial-and-error. For example, cross-validation
is a popular choice for model selection, involving training of a number
of randomly initialized models. NAS addresses this challenge by
automating the discovery of high-performing architectures within a
predefined search space <span class="citation"
data-cites="poyser_2024_nas-review">(Poyser &amp; Breckon, 2024)</span>.
Most methods for NAS incur a high search cost in the form of (partially)
training the architectures to score them, and/or training a *search
model* that can make the architecture search efficient. Recently, there
has been a increased focus on cheapening the search process, while
retaining or improving the quality of the selected architectures. Some
of these are NTK-based methods like TE-NAS <span class="citation"
data-cites="chen2020tenas">(Chen, Gong, &amp; Wang, 2021)</span>, which
uses the condition number of the NTK Gram Matrix, and KNAS <span
class="citation" data-cites="pmlr-v139-xu21m">(Xu et al., 2021)</span>,
which uses the mean of the Matrix’s entries. These studies report
competitive performances on real-world computer vision tasks in NAS
benchmarks <span class="citation"
data-cites="liu2018darts ying19nasbench101 dong2020nasbench201">(Dong
&amp; Yang, 2020; Liu et al., 2019; Ying et al., 2019)</span>.</p>
<p>Citing the success of NTK-based NAS methods that use correlation
between architecture scores and the corresponding test accuracy as a
measure of their strategy’s efficacy, we aim to propose a scoring method
that is closely correlated to the training loss. As with other NTK-based
methods, it will utilize the Gram matrix associated with the NTK, but
for the purpose of computing the kernel canonical correlation (KCC)
<span class="citation" data-cites="akaho2007kcca thomas2001kcca">(Akaho,
2007; Melzer et al., 2001)</span> between the inputs and the
outputs.</p>
<h1 id="theory">Theory</h1>
<p>For simplicity, we assume that we have a univariate regression task
at hand. Consider the set of neural network functions, <span
class="math inline">\(\mathcal{A}\)</span>, parameterized by some
architecture, <span class="math inline">\(\mathbf{A}\)</span>. Most
neural networks designed for regression have a linear output layer, and
we assume the same for <span class="math inline">\(\mathbf{A}\)</span>.
In that case, minimizing the Mean Squared Error (MSE) between the
network output and the regression labels, is equivalent to maximizing
their correlation, since the parameters of the output layer can be
adjusted post-training to get the minimizer of the MSE <span
class="citation" data-cites="englisch_1994_corr-mse">(Englisch &amp;
Hiemstra, 1994)</span>. Accordingly, we define the score for
architecture <span class="math inline">\(\mathbf{A}\)</span> as</p>
<p><span class="math display">\[\begin{aligned}
  S\left(\mathbf{A}\right)
  \coloneqq \max_{f\in\mathcal{A}}
\text{Corr}\left(f\left(\mathbf{X}\right), \mathbf{Y}\right)
  = \max_{f\in\mathcal{A}, g\in\mathcal{L}}
\text{Corr}\left(f\left(\mathbf{X}\right),
g\left(\mathbf{Y}\right)\right)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mathcal{L}\)</span> is the space
of linear functions on <span class="math inline">\(\mathbf{Y}\)</span>.
This optimization is, in general, hard to perform. &lt;!– Furthermore,
it is somewhat unprincipled since the subset of functions accessible to
gradient descent is not the entirety of <span
class="math inline">\(\mathcal{A}\)</span>. –&gt; Instead, if we were to
optimize over some RKHS, then the optimization is equivalent to
performing kernel canonical correlation analysis (KCCA) with the
associated reproducing kernel on <span
class="math inline">\(\mathbf{X}\)</span> and the linear kernel on <span
class="math inline">\(\mathbf{Y}\)</span>. Accordingly, we seek an RKHS
that can approximate the space of functions the network can converge
to.</p>
<h2 id="neural-tangent-kernel">Neural Tangent Kernel</h2>
<p>Consider an <span class="math inline">\(L\)</span>-layer
fully-connected feed-forward network under the NTK parameterization:</p>
<p><span class="math display">\[\begin{aligned}
  \mathbf{x}^{l+1} = \mathbf{W}^{l+1}\mathbf{x}^l + \mathbf{b}^{l+1}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(W_{ij} \sim \mathcal{N}\left(0,
\sigma_w^2/n_l\right)\)</span> and <span class="math inline">\(b_i \sim
\mathcal{N}\left(0, \sigma_b^2\right)\)</span>, <span
class="math inline">\(n_l\)</span> being the width of layer <span
class="math inline">\(l\)</span>. We define <span
class="math inline">\(\mathbf{\theta}^l \coloneqq
\text{vec}\left(\left\{W^l,b^l\right\}\right)\)</span> as the collection
of parameters in layer <span class="math inline">\(l\)</span>, and <span
class="math inline">\(\mathbf{\theta} \coloneqq
\text{vec}\left(\cup_{l=1}^L \mathbf{\theta}^l \right)\)</span> as the
collection of all parameters.</p>
<p>The parameter dynamics and the predictive dynamics for this model
under gradient flow can be written as:</p>
<p><span class="math display">\[\begin{aligned}
  \dot{\theta}_t &amp;= -\eta \nabla_{\theta}
\mathcal{L}\left(\mathcal{D}; \theta_t\right) = -\eta \nabla_{\theta}
f\left(\mathbf{X}; \theta_t\right)^T \nabla_{f} \mathcal{L}(\mathcal{D};
\theta_t) \\
  \dot{f}(\mathbf{X}; \theta_t) &amp;=
\nabla_{\theta}f(\mathbf{X};\theta_t) \dot{\theta}_t = -\eta
\underbrace{\nabla_{\theta} f(\mathbf{X}; \theta_t) \nabla_{\theta}
f(\mathbf{X}; \theta_t)^T}_{\triangleq \hat{\Theta}_t(\mathbf{X},
\mathbf{X})} \nabla_{f} \mathcal{L}(\mathcal{D}; \theta_t)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mathcal{D}\)</span> is the
training data set, <span class="math inline">\(\mathcal{L}\)</span> is
the loss function, <span class="math inline">\(\eta\)</span> is the
learning rate, and <span class="math inline">\(\hat{\Theta}_t\)</span>
is the Empirical Neural Tangent Kernel (NTK) <span class="citation"
data-cites="jacot_2018_ntk">(Jacot, Gabriel, &amp; Hongler,
2018)</span>.</p>
<p>In the infinite-width limit, the NTK converges in distribution to an
analytical limit, <span class="math inline">\(\Theta\)</span>, and the
NNs evolve as linear models <span class="citation"
data-cites="lee_2019_wide-nets-linear">(Lee et al., 2019)</span>. Under
gradient flow, the predictive distribution of this wide network
converges to a normal distribution <span class="citation"
data-cites="lee_2019_wide-nets-linear">(Lee et al., 2019)</span>, <span
class="math inline">\(f^{\text{lin}}_{\theta_{\infty}}\left(x\right)
\sim
\mathcal{N}\left(\mu_{\text{NN}}\left(x\right),\Sigma_{\text{NN}}\left(x,x\right)\right)\)</span>,
where</p>
<p><span class="math display">\[\begin{aligned}
  &amp;\mu_{\text{NN}}\left(x\right) = \Theta\left(x,\mathbf{X}\right)
\Theta\left(\mathbf{X}\right)^{-1} \mathbf{Y} \\
  &amp;\begin{split}
    \Sigma_{\text{NN}}\left(x,x&#39;\right) &amp;=
\mathcal{K}\left(x,x&#39;\right) + \Theta\left(x,\mathbf{X}\right)
\Theta\left(\mathbf{X}\right)^{-1} \mathcal{K}\left(\mathbf{X}\right)
\Theta\left(\mathbf{X}\right)^{-1} \Theta\left(\mathbf{X},x&#39;\right)
\\
    &amp;\quad - \left(\Theta\left(x,\mathbf{X}\right)
\Theta\left(\mathbf{X}\right)^{-1}
\mathcal{K}\left(\mathbf{X},x&#39;\right) +
    \mathcal{K}\left(x&#39;,\mathbf{X}\right)
\Theta\left(\mathbf{X}\right)^{-1}
\Theta\left(\mathbf{X},x\right)\right)
  \end{split}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mathcal{K}\)</span> denotes the
NN-GP kernel <span class="citation"
data-cites="matthews2018gaussianprocessbehaviourwide">(G. Matthews,
Rowland, Hron, Turner, &amp; Ghahramani, 2018)</span>, defined as <span
class="math inline">\(\mathcal{K}\left(x,x&#39;\right) = \mathbb{E}
\left[f_{\theta}\left(x\right) \cdot
f_{\theta}\left(x&#39;\right)\right]\)</span> which also converges in
the infinite-width limit.</p>
<p>The covariance <span
class="math inline">\(\Sigma_{\text{NN}}\)</span> is inconvenient to
deal with, involving two computationally expensive kernel computations,
and a series of cubic-time matrix operations. To tackle this, we can
augment the forward pass (denoted by <span
class="math inline">\(\tilde{f}_{\theta}\)</span>) by adding a random,
untrainable function, which results in the distribution at convergence
having a GP-posterior-like form, with <span
class="math inline">\(\Theta\)</span> as the covariance kernel <span
class="citation" data-cites="bobby_2020_bayesian-ensembles-ntk">(He,
Lakshminarayanan, &amp; Teh, 2020)</span>, <span
class="math inline">\(\tilde{f}_{\theta_{\infty}} \sim
\mathcal{N}\left(\mu_{\text{NTK}},\Sigma_{\text{NTK}}\right)\)</span>,
where <span class="math inline">\(\mu_{\text{NTK}} =
\mu_{\text{NN}}\)</span> and:</p>
<p><span class="math display">\[\begin{aligned}
    \Sigma_{\text{NTK}}\left(x,y\right) = \Theta\left(x,x&#39;\right) -
\Theta\left(x,\mathbf{X}\right) \Theta\left(\mathbf{X}\right)^{-1}
\Theta\left(\mathbf{X},x&#39;\right)
\end{aligned}\]</span></p>
<p>Importantly, in my Bachelor’s thesis project <span class="citation"
data-cites="hemachandra23a">(Hemachandra, Dai, Singh, Ng, &amp; Low,
2023)</span>, we showed that the ratio between <span
class="math inline">\(\Sigma_{\text{NN}}\left(x,x&#39;\right)\)</span>
and <span
class="math inline">\(\Sigma_{\text{NTK}}\left(x,x&#39;\right)\)</span>
can be tightly upper bounded, and hence, the NTK-GP posterior, <span
class="math inline">\(\mathcal{N}\left(\mu_{\text{NTK-GP}}\left(x\right),\Sigma_{\text{NTK-GP}}\left(x,x\right)\right)\)</span>,
may be considered a reasonable approximation for the predictive
distribution, <span
class="math inline">\(\mathcal{N}\left(\mu_{\text{NN}}\left(x\right),\Sigma_{\text{NN}}\left(x,x\right)\right)\)</span>.</p>
<h2 id="kernel-canonical-correlation-analysis">Kernel Canonical
Correlation Analysis</h2>
<p>We now consider the more practical architectures which have finite
width, so that the feature mapping, <span class="math inline">\(x
\mapsto \nabla_{\theta}f_{\theta}\left(x\right)\)</span>, associated
with the empirical NTK, <span class="math inline">\(\Theta\)</span>, is
finite dimensional. Hence, the samples from the NTK-GP prior are
almost-surely contained in the RKHS, <span
class="math inline">\(\mathcal{H}_{\Theta}\)</span>, associated with
<span class="math inline">\(\Theta\)</span> (&lt;span
style="color:red"&gt;not sure about this part&lt;/span&gt;). Since the
GP posterior does not have support where the prior does not, the
posterior samples are also contained in this RKHS. Therefore, we can use
the RKHS associated with the NTK to compute the KCC:</p>
<p><span class="math display">\[\begin{aligned}
    S\left(\mathbf{A}\right)
    \approx \max_{f\in\mathcal{H}_{\Theta}, g\in\mathcal{L}}
\text{Corr}\left(f\left(\mathbf{X}\right),
g\left(\mathbf{Y}\right)\right)
\end{aligned}\]</span></p>
<p>This value is trivially equal to <span class="math inline">\(\pm
1\)</span> when the kernel matrices associated with <span
class="math inline">\(\mathbf{X}\)</span> and <span
class="math inline">\(\mathbf{Y}\)</span> are full-rank <span
class="citation" data-cites="gretton05a">(Gretton, Herbrich, Smola,
Bousquet, &amp; Schölkopf, 2005)</span>. A common practice is to add
some regularization to this problem by penalizing rougher witness
functions, <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span>, which yields the following
generalized-eigenvalue problem:</p>
<p><span class="math display">\[\begin{aligned}
    \begin{bmatrix}
        0 &amp; \tilde{\Theta}\tilde{L} \\
        \tilde{\mathbf{L}}\tilde{\Theta} &amp; 0
    \end{bmatrix} \mathbf{u} =
    \lambda
    \begin{bmatrix}
        \tilde{\Theta}^2 + m\epsilon\tilde{\Theta} &amp; 0 \\
        0 &amp; \tilde{\mathbf{L}}^2 + m\epsilon\tilde{\mathbf{L}}
    \end{bmatrix} \mathbf{u}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{\Theta} =
\mathbf{H}\Theta\left(\mathbf{X}\right)\mathbf{H}\)</span> and <span
class="math inline">\(\tilde{\mathbf{L}} =
\mathbf{H}\mathbf{Y}\mathbf{Y}^T\mathbf{H}\)</span> are the centered
Gram matrices, <span class="math inline">\(\mathbf{H} = \mathbf{I}_m -
1/m \cdot \mathbf{1}_{m\times m}\)</span> is the centering matrix, <span
class="math inline">\(m\)</span> is the number of data points and <span
class="math inline">\(\epsilon\)</span> is the regularization constant.
The regularized canonical correlation is the maximum eigenvalue of this
problem, <span class="math inline">\(\gamma =
\lambda_{\text{max}}\)</span>.</p>
<h1 id="limitations-and-extensions">Limitations and Extensions</h1>
<p>The proposed scoring function is expected to be over-confident for
two reasons:</p>
<ol>
<li><p>Using <span
class="math inline">\(\Theta\left(\mathbf{X}\right)\)</span> means that
we are optimizing the correlation over the NTK-GP prior’s support, which
is larger than the posterior’s support.</p></li>
<li><p>The witness function, <span class="math inline">\(f\)</span>,
used for computing the canonical correlation is the best NN fitting the
training set. This amounts to ignoring the probabilistic information in
the prior/posterior altogether. Therefore, it could represent an
over-fitting scenario.</p></li>
<li><p>Using a linear kernel might make sense for regression, but needs
justification for classification tasks. What even is an appropriate
kernel in the classification case? This is an important issue because
most NAS benchmarks involve image classification tasks, like
CIFAR-10/100 and ImageNet.</p></li>
</ol>
<p>Another limitation is the lack of interpretability of KCC, which is
crucial in many real-world applications. To address this limitation, we
may explore alternate kernel-based measures, such as those based on
Hilbert-Schmidt Independence Criterion (HSIC) <span class="citation"
data-cites="gretton2005hsic">(Gretton, Bousquet, Smola, &amp; Schölkopf,
2005)</span>, and the Kernel Target Alignment (KTA) <span
class="citation" data-cites="cortes12kta">(Cortes, Mohri, &amp;
Rostamizadeh, 2012)</span>, as proposed in <span class="citation"
data-cites="chang13hsic">(Chang, Kruger, Kustra, &amp; Zhang,
2013)</span>.</p>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-akaho2007kcca" class="csl-entry" role="listitem">
Akaho, S. (2007). A kernel method for canonical correlation analysis.
Retrieved from <a
href="https://arxiv.org/abs/cs/0609071">https://arxiv.org/abs/cs/0609071</a>
</div>
<div id="ref-chang13hsic" class="csl-entry" role="listitem">
Chang, B., Kruger, U., Kustra, R., &amp; Zhang, J. (2013). Canonical
correlation analysis based on hilbert-schmidt independence criterion and
centered kernel target alignment. In <em>Proceedings of the 30th
international conference on machine learning</em> (Vol. 28, pp.
316–324). Atlanta, Georgia, USA: PMLR. Retrieved from <a
href="https://proceedings.mlr.press/v28/chang13.html">https://proceedings.mlr.press/v28/chang13.html</a>
</div>
<div id="ref-chen2020tenas" class="csl-entry" role="listitem">
Chen, W., Gong, X., &amp; Wang, Z. (2021). Neural architecture search on
ImageNet in four GPU hours: A theoretically inspired perspective. In
<em>International conference on learning representations</em>.
</div>
<div id="ref-cortes12kta" class="csl-entry" role="listitem">
Cortes, C., Mohri, M., &amp; Rostamizadeh, A. (2012). Algorithms for
learning kernels based on centered alignment. <em>Journal of Machine
Learning Research</em>, <em>13</em>(28), 795–828. Retrieved from <a
href="http://jmlr.org/papers/v13/cortes12a.html">http://jmlr.org/papers/v13/cortes12a.html</a>
</div>
<div id="ref-dong2020nasbench201" class="csl-entry" role="listitem">
Dong, X., &amp; Yang, Y. (2020). NAS-bench-201: Extending the scope of
reproducible neural architecture search. In <em>International conference
on learning representations (ICLR)</em>. Retrieved from <a
href="https://openreview.net/forum?id=HJxyZkBKDr">https://openreview.net/forum?id=HJxyZkBKDr</a>
</div>
<div id="ref-englisch_1994_corr-mse" class="csl-entry" role="listitem">
Englisch, H., &amp; Hiemstra, Y. (1994). The correlation as cost
function in neural networks. In <em>Proceedings of 1994 IEEE
international conference on neural networks (ICNN’94)</em> (Vol. 5, pp.
3170–3172 vol.5). doi:<a
href="https://doi.org/10.1109/ICNN.1994.374741">10.1109/ICNN.1994.374741</a>
</div>
<div id="ref-matthews2018gaussianprocessbehaviourwide" class="csl-entry"
role="listitem">
G. Matthews, A. G. de, Rowland, M., Hron, J., Turner, R. E., &amp;
Ghahramani, Z. (2018). Gaussian process behaviour in wide deep neural
networks. Retrieved from <a
href="https://arxiv.org/abs/1804.11271">https://arxiv.org/abs/1804.11271</a>
</div>
<div id="ref-gretton2005hsic" class="csl-entry" role="listitem">
Gretton, A., Bousquet, O., Smola, A., &amp; Schölkopf, B. (2005).
Measuring statistical dependence with hilbert-schmidt norms. In S. Jain,
H. U. Simon, &amp; E. Tomita (Eds.), <em>Algorithmic learning
theory</em> (pp. 63–77). Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-gretton05a" class="csl-entry" role="listitem">
Gretton, A., Herbrich, R., Smola, A., Bousquet, O., &amp; Schölkopf, B.
(2005). Kernel methods for measuring independence. <em>Journal of
Machine Learning Research</em>, <em>6</em>(70), 2075–2129. Retrieved
from <a
href="http://jmlr.org/papers/v6/gretton05a.html">http://jmlr.org/papers/v6/gretton05a.html</a>
</div>
<div id="ref-bobby_2020_bayesian-ensembles-ntk" class="csl-entry"
role="listitem">
He, B., Lakshminarayanan, B., &amp; Teh, Y. W. (2020). Bayesian deep
ensembles via the neural tangent kernel. In <em>Advances in neural
information processing systems</em> (Vol. 33, pp. 1010–1022). Curran
Associates, Inc. Retrieved from <a
href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf</a>
</div>
<div id="ref-hemachandra23a" class="csl-entry" role="listitem">
Hemachandra, A., Dai, Z., Singh, J., Ng, S.-K., &amp; Low, B. K. H.
(2023). Training-free neural active learning with
initialization-robustness guarantees. In <em>Proceedings of the 40th
international conference on machine learning</em> (Vol. 202, pp.
12931–12971). PMLR. Retrieved from <a
href="https://proceedings.mlr.press/v202/hemachandra23a.html">https://proceedings.mlr.press/v202/hemachandra23a.html</a>
</div>
<div id="ref-jacot_2018_ntk" class="csl-entry" role="listitem">
Jacot, A., Gabriel, F., &amp; Hongler, C. (2018). Neural tangent kernel:
Convergence and generalization in neural networks. In <em>Advances in
neural information processing systems</em> (Vol. 31). Curran Associates,
Inc. Retrieved from <a
href="https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf</a>
</div>
<div id="ref-lee_2019_wide-nets-linear" class="csl-entry"
role="listitem">
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein,
J., &amp; Pennington, J. (2019). Wide neural networks of any depth
evolve as linear models under gradient descent. In <em>Advances in
neural information processing systems</em> (Vol. 32). Curran Associates,
Inc. Retrieved from <a
href="https://proceedings.neurips.cc/paper_files/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf</a>
</div>
<div id="ref-liu2018darts" class="csl-entry" role="listitem">
Liu, H., Simonyan, K., &amp; Yang, Y. (2019). <span>DARTS</span>:
Differentiable architecture search. In <em>International conference on
learning representations</em>. Retrieved from <a
href="https://openreview.net/forum?id=S1eYHoC5FX">https://openreview.net/forum?id=S1eYHoC5FX</a>
</div>
<div id="ref-thomas2001kcca" class="csl-entry" role="listitem">
Melzer, T., Reiter, M., &amp; Bischof, H. (2001). Nonlinear feature
extraction using generalized canonical correlation analysis. In G.
Dorffner, H. Bischof, &amp; K. Hornik (Eds.), <em>Artificial neural
networks — ICANN 2001</em> (pp. 353–360). Berlin, Heidelberg: Springer
Berlin Heidelberg.
</div>
<div id="ref-poyser_2024_nas-review" class="csl-entry" role="listitem">
Poyser, M., &amp; Breckon, T. P. (2024). Neural architecture search: A
contemporary literature review for computer vision applications.
<em>Pattern Recognition</em>, <em>147</em>, 110052. doi:<a
href="https://doi.org/10.1016/j.patcog.2023.110052">https://doi.org/10.1016/j.patcog.2023.110052</a>
</div>
<div id="ref-pmlr-v139-xu21m" class="csl-entry" role="listitem">
Xu, J., Zhao, L., Lin, J., Gao, R., Sun, X., &amp; Yang, H. (2021).
KNAS: Green neural architecture search. In M. Meila &amp; T. Zhang
(Eds.), <em>Proceedings of the 38th international conference on machine
learning</em> (Vol. 139, pp. 11613–11625). PMLR. Retrieved from <a
href="https://proceedings.mlr.press/v139/xu21m.html">https://proceedings.mlr.press/v139/xu21m.html</a>
</div>
<div id="ref-ying19nasbench101" class="csl-entry" role="listitem">
Ying, C., Klein, A., Christiansen, E., Real, E., Murphy, K., &amp;
Hutter, F. (2019). <span>NAS</span>-bench-101: Towards reproducible
neural architecture search. In <em>Proceedings of the 36th international
conference on machine learning</em> (Vol. 97, pp. 7105–7114). Long
Beach, California, USA: PMLR. Retrieved from <a
href="http://proceedings.mlr.press/v97/ying19a.html">http://proceedings.mlr.press/v97/ying19a.html</a>
</div>
</div>
</body>
</html>
